{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "974f62b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import netCDF4 as nc\n",
    "import gsw\n",
    "from datetime import datetime,date\n",
    "import os as os\n",
    "from matplotlib.gridspec import GridSpec\n",
    "from numpy.fft import fft,fft2,fftfreq\n",
    "# importig movie py libraries\n",
    "# from moviepy.editor import VideoClip\n",
    "from scipy.interpolate import interp2d\n",
    "# from moviepy.video.io.bindings import mplfig_to_npimage\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from scipy import fftpack\n",
    "from tqdm import tqdm\n",
    "# analog data assimilation\n",
    "from scipy.stats import linregress,norm\n",
    "import dask as da\n",
    "import xarray as xar\n",
    "# from pydmd import DMD\n",
    "import glob as glob\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98988c9f-59a3-4355-a6d3-365f9272e6c5",
   "metadata": {},
   "source": [
    "# The downloaded OCCIPUT files are modified according to our desired format, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a2f720ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "File=xar.open_mfdataset(glob.glob('Pacific_OCCIPUT*s.nc'))\n",
    "Filemod=File.rename({'deptht':'depth','time_counter':'time','votemper':'TEMP','vosaline':'PSAL',\"sossheig\":\"SSH\"})\n",
    "Filemod=Filemod.assign({\"latitude\":Filemod['latitude'].isel(time=0).mean('x'),\"longitude\":Filemod['longitude'].where(Filemod['longitude']>0,Filemod['longitude']+360).isel(time=0).mean('y'),})\n",
    "Filemod=Filemod.drop(['time_centered_bounds','time_counter_bounds','time_centered']).swap_dims({\"x\":\"longitude\",'y':'latitude'})\n",
    "Filemod=Filemod.assign({'longitude':Filemod['longitude'].where(np.where(Filemod['longitude'])[0]!=60., -179.81245+360)})\n",
    "\n",
    "Filemod.to_netcdf('Pacific_OCCIPUT_Full.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b037f94b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Filemod=xar.open_mfdataset('Pacific_OCCIPUT_Full.nc')\n",
    "Filemod=Filemod.where(Filemod['longitude']<=290,drop=True)\n",
    "Filemod=Filemod.where((Filemod.isnull().rolling({'time':4},center=True,min_periods=2).sum('time').isin([0,1,2,3])).all('time')).interpolate_na('time',limit=3)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f2af48d-9c14-4450-b17c-2d34acdac301",
   "metadata": {},
   "source": [
    "# The gridded temperature and salinity fields are coarsed down to 3° cells. A climatology is evaluated over the 1995-2015 period. The fields are saved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "995f5695",
   "metadata": {},
   "outputs": [],
   "source": [
    "RC = Filemod.coarsen({'longitude':3,'latitude':3},boundary=\"trim\",side='left').mean().transpose(\"time\",\"depth\",\"latitude\",\"longitude\")\n",
    "\n",
    "JULTIME = (RC['time'].data.astype('datetime64[D]')-np.datetime64('1950-01-01')).astype('timedelta64[D]').astype('float64')\n",
    "\n",
    "RC = RC.assign({'time2':(JULTIME)})\n",
    "\n",
    "RC.coords['time2']=RC.coords['time']\n",
    "RC.coords['time']=JULTIME\n",
    "\n",
    "\n",
    "\n",
    "RC = RC.merge(RC[['TEMP','PSAL']].where(RC['time2'].dt.year>=1995,drop=True).groupby(\"time2.month\").mean(skipna=False).rename({'TEMP':'T_Climato','PSAL':'S_Climato'}))\n",
    "\n",
    "\n",
    "res2 = 9\n",
    "\n",
    "RC = RC.merge((RC[['TEMP','PSAL']].where(RC['time2'].dt.year>=1995,drop=True).groupby('time2.month')-RC[['TEMP','PSAL']].where(RC['time2'].dt.year>=1995,drop=True).groupby('time2.month').mean(skipna=False)).polyfit(deg=1,dim='time'))\n",
    "              \n",
    "\n",
    "RC.to_netcdf('Pacific_OCCIPUT_Full2.nc','w',format='NETCDF4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "284e5647",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2161,
   "id": "6d1b379b",
   "metadata": {},
   "outputs": [],
   "source": [
    "RC=xar.open_mfdataset('Pacific_OCCIPUT_Full2.nc')\n",
    "RC=RC.where(RC['time2'].dt.year>=1995,drop=True).isel(depth=[0,3,4,5,6,7,8])\n",
    "\n",
    "RC['TEMP_polyfit_coefficients'] = RC['TEMP_polyfit_coefficients'].isel(time=0)\n",
    "RC['T_Climato'] = RC['T_Climato'].isel(time=0)\n",
    "RC['S_Climato'] = RC['S_Climato'].isel(time=0)\n",
    "RC['PSAL_polyfit_coefficients'] = RC['PSAL_polyfit_coefficients'].isel(time=0)\n",
    "\n",
    "# RC = RC.assign({'TEMP_polyfit_coefficients':RC['TEMP_polyfit_coefficients'].where(RC['TEMP'].isel(time=0).notnull()),'PSAL_polyfit_coefficients':RC['PSAL_polyfit_coefficients'].where(RC['PSAL'].isel(time=0).notnull())})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d4bb0c1-d4b1-47a9-9586-4f82aa935e56",
   "metadata": {},
   "source": [
    "# Temperature:  The files of the synthetic observations are opened. They are then standardized over the desired depth levels. \n",
    "# To evaluate the datasets for all the decades, timemin and timemax have to be modified manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "4998ad9b-4912-4b93-adad-80e2b7b8a8aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "10\n",
      "50\n",
      "100\n",
      "300\n",
      "500\n",
      "700\n",
      "1000\n",
      "1500\n",
      "2000\n"
     ]
    }
   ],
   "source": [
    "timemin=1990\n",
    "timemax=timemin+9\n",
    "ENtemp = xar.open_mfdataset('ENtemp'+str(timemin)[-2:]+'Pacific_OCCIPUT.nc',parallel=True)\n",
    "\n",
    "switch = False \n",
    "for zepth in [5,10,50,100,300,500,700,1000,1500,2000]:\n",
    "    \n",
    "    if zepth == 2000:\n",
    "        bnd1,bnd2 = 1750,2250\n",
    "        \n",
    "    if zepth == 1500:\n",
    "        bnd1,bnd2 = 1250,1750\n",
    "        \n",
    "    if zepth == 1000:\n",
    "        bnd1,bnd2 = 850,1250\n",
    "    elif (zepth==700):\n",
    "        bnd1,bnd2 = 600,800\n",
    "    elif (zepth==500):\n",
    "        bnd1,bnd2 = 450,600\n",
    "    elif (zepth==400):\n",
    "        bnd1,bnd2 = 350,450\n",
    "    elif (zepth==300):\n",
    "        bnd1,bnd2 = 250,350\n",
    "    elif (zepth==200):\n",
    "        bnd1,bnd2 = 150,250\n",
    "    elif (zepth==100):\n",
    "        bnd1,bnd2 = 75,150\n",
    "    elif (zepth==50):\n",
    "        bnd1,bnd2 = 25,75\n",
    "    elif (zepth==10):\n",
    "        bnd1,bnd2 = 7,25\n",
    "    else:\n",
    "        bnd1,bnd2 = 0,7\n",
    "        \n",
    "    print(zepth)\n",
    "    \n",
    "    \n",
    "    argumt=da.array.argmin(da.array.where(zepth-ENtemp['DEPTH'].fillna(1e5)>=0,zepth-ENtemp['DEPTH'].fillna(1e5),1e5),1).compute()\n",
    "    born1=da.array.all(zepth-ENtemp['DEPTH'].ffill(dim='N_LEVELS').bfill(dim='N_LEVELS')<0,axis=1)\n",
    "    born2=da.array.all(zepth-ENtemp['DEPTH'].ffill(dim='N_LEVELS').bfill(dim='N_LEVELS')>0,axis=1)\n",
    "\n",
    "    Nprs = ENtemp.coords['N_PROF'].size\n",
    "    Nzz = ENtemp.coords['N_LEVELS'].size\n",
    "\n",
    "    z0,z1=ENtemp['DEPTH'].data.vindex[np.linspace(0,Nprs-1,Nprs,dtype='int'),argumt],ENtemp['DEPTH'].data.vindex[np.linspace(0,Nprs-1,Nprs,dtype='int'),np.where((1+argumt)>149,149,(1+argumt))]\n",
    "    quality = (ENtemp['POTM_QC']==1.)&(ENtemp['POTM_LEVEL_QC']<=2.)\n",
    "    T0,T1=ENtemp['POTM'].where(quality).data.vindex[np.linspace(0,Nprs-1,Nprs,dtype='int'),argumt],ENtemp['POTM'].where(quality).data.vindex[np.linspace(0,Nprs-1,Nprs,dtype='int'),np.where((1+argumt)>149,149,(1+argumt))]\n",
    "\n",
    "    born3 = np.array([born1, born2, (z0 <bnd1), (z1>bnd2) ,(z0 > z1) ,(zepth<z0) ,(zepth>z1)]).any(axis=0)\n",
    "\n",
    "\n",
    "\n",
    "    #     born3 = (z0-z1>=zscl)\n",
    "    T0=np.where(born3,np.nan,T0)\n",
    "    T1=np.where(born3,np.nan,T1)\n",
    "    if switch : \n",
    "        ENewtemp = np.concatenate((ENewtemp,(np.array([T0,T1,np.empty_like(T0)])[np.nanargmin(np.abs(np.array([z0,z1,1e6*np.ones_like(T0)])-zepth),0),np.ix_(np.arange(0,Nprs,1))])),axis=0)\n",
    "        # QC_z = np.concatenate((QC_z,np.where(np.isnan(T0),np.nan,(np.where(z1-z0<=zscl,0,1)+np.where(zepth-z0<zscl,0,1)+np.where(z1-zepth,0,1)))[np.newaxis,:]),axis=0)\n",
    "    if not switch: \n",
    "        ENewtemp = np.array([T0,T1,np.empty_like(T0)])[np.nanargmin(np.abs(np.array([z0,z1,1e6*np.ones_like(T0)])-zepth),0),np.ix_(np.arange(0,Nprs,1))]\n",
    "        # QC_z = np.where(np.isnan(T0),np.nan,(np.where(z1-z0<=zscl,0,1)+np.where(zepth-z0<zscl,0,1)+np.where(z1-zepth,0,1)))[np.newaxis,:]\n",
    "        switch = True\n",
    "\n",
    "       #\n",
    "#      \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f222f53-1df6-461a-b3b4-4ffabfe80cbe",
   "metadata": {},
   "source": [
    "# The super-profiles are created for obserations with daily frequency measurements (moorings, mostly). The observation files are saved. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "49adb64c-2293-41ab-89ab-133ec3acff84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "137/(138,)\r"
     ]
    }
   ],
   "source": [
    "\n",
    "ENtemp2 = xar.Dataset( \n",
    "        {\n",
    "            'POTM': (['depth','N_PROF'],ENewtemp),\n",
    "            # 'QC' : (['depth','N_PROF'],QC_z.compute())\n",
    "        },\n",
    "        coords = {\n",
    "            'JULD': (['N_PROF'],ENtemp['JULD'].values),\n",
    "            'LATITUDE': (['N_PROF'],ENtemp['LATITUDE'].values),\n",
    "            'LONGITUDE': (['N_PROF'],ENtemp['LONGITUDE'].values),\n",
    "            'Depth': (['depth'],RC['depth'].values),\n",
    "            'STATION_IDENTIFIER' : (['N_PROF'],ENtemp['STATION_IDENTIFIER'].values),\n",
    "            'STATION_NUMBER' : (['N_PROF'], ENtemp['STATION_TYPE'].values)\n",
    "        }\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "indexes = np.char.array(np.linspace(0,500,501,dtype='int').astype('str') ).zfill(3)\n",
    "\n",
    "indexes = np.char.array(['820'] ).zfill(3)\n",
    "\n",
    "condit = ((ENtemp2['STATION_NUMBER'].dropna('N_PROF').str.strip().astype('str').isin(indexes)))\n",
    "\n",
    "\n",
    "moorObs = ENtemp2.where(condit,drop=True).reset_coords(['LATITUDE','LONGITUDE']).compute()\n",
    "\n",
    "labofmoor = np.unique(moorObs['STATION_IDENTIFIER'])\n",
    "\n",
    "timebins=np.arange(np.datetime64(str(timemin)+'-01-01'),np.datetime64(str(timemax)+'-12-31')+15,15)\n",
    "# timebins=np.arange(np.datetime64('1990-01-01'),np.datetime64('1999-12-31')+15,15)\n",
    "\n",
    "if labofmoor.size!=0:\n",
    "    moorObs2 = moorObs.where(moorObs['STATION_IDENTIFIER']==labofmoor[0],drop=True).groupby_bins(group='JULD',bins=timebins).mean(skipna=True).dropna('JULD_bins',how='all').rename_dims({'JULD_bins':'N_PROF'})\n",
    "    moorObs2 = moorObs2.assign_coords({'JULD_bins':(['N_PROF'],np.asanyarray([_.mid for _ in(moorObs2['JULD_bins'].values)]))})\n",
    "\n",
    "    ii=1\n",
    "    for label in labofmoor[1:] : \n",
    "        print(str(ii)+'/'+str(labofmoor.shape),end='\\r')\n",
    "        appendix=moorObs.where(moorObs['STATION_IDENTIFIER']==label,drop=True).groupby_bins(group='JULD',bins=timebins).mean(skipna=True).dropna('JULD_bins',how='all').rename_dims({'JULD_bins':'N_PROF'})\n",
    "        appendix = appendix.assign_coords({'JULD_bins':(['N_PROF'],np.asanyarray([_.mid for _ in(appendix['JULD_bins'].values)]))})\n",
    "\n",
    "        moorObs2 =xar.concat((moorObs2,appendix),dim='N_PROF')\n",
    "        ii+=1\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    ENtemp3 = xar.concat((ENtemp2.where(~condit,drop=True).reset_coords(['LATITUDE','LONGITUDE']).drop(['STATION_NUMBER','STATION_IDENTIFIER']),moorObs2.rename({'JULD_bins':'JULD'})),dim='N_PROF')\n",
    "    \n",
    "else : \n",
    "    ENtemp3 =ENtemp2.where(~condit,drop=True).reset_coords(['LATITUDE','LONGITUDE']).drop(['STATION_NUMBER','STATION_IDENTIFIER'])\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "ENtemp3.to_netcdf('ENtemp_NoInterp'+str(timemin)[-2:]+'NoFMoor_OCCIPUT.nc','w')\n",
    "ENtemp2.to_netcdf('ENtemp_NoInterp'+str(timemin)[-2:]+'_OCCIPUT.nc','w')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "361f7b8b-0ebc-49e9-b8f3-5abf6d5fd77a",
   "metadata": {},
   "source": [
    "# Salinity:  The files of the synthetic observations are opened. They are then standardized over the desired depth levels. \n",
    "# To evaluate the datasets for all the decades, timemin and timemax have to be modified manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "ea05e860-c862-42fe-a770-20b7abed07be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "10\n",
      "50\n",
      "100\n",
      "300\n",
      "500\n",
      "700\n",
      "1000\n",
      "1500\n",
      "2000\n"
     ]
    }
   ],
   "source": [
    "timemin=1990\n",
    "timemax=timemin+9\n",
    "ENsal = xar.open_mfdataset('ENsal'+str(timemin)[-2:]+'Pacific_OCCIPUT.nc',parallel=True)\n",
    "\n",
    "switch = False \n",
    "for zepth in [5,10,50,100,300,500,700,1000,1500,2000]:\n",
    "    \n",
    "    if zepth == 2000:\n",
    "        bnd1,bnd2 = 1750,2250\n",
    "        \n",
    "    if zepth == 1500:\n",
    "        bnd1,bnd2 = 1250,1750\n",
    "        \n",
    "    if zepth == 1000:\n",
    "        bnd1,bnd2 = 850,1250\n",
    "    elif (zepth==700):\n",
    "        bnd1,bnd2 = 600,800\n",
    "    elif (zepth==500):\n",
    "        bnd1,bnd2 = 450,600\n",
    "    elif (zepth==400):\n",
    "        bnd1,bnd2 = 350,450\n",
    "    elif (zepth==300):\n",
    "        bnd1,bnd2 = 250,350\n",
    "    elif (zepth==200):\n",
    "        bnd1,bnd2 = 150,250\n",
    "    elif (zepth==100):\n",
    "        bnd1,bnd2 = 75,150\n",
    "    elif (zepth==50):\n",
    "        bnd1,bnd2 = 25,75\n",
    "    elif (zepth==10):\n",
    "        bnd1,bnd2 = 7,25\n",
    "    else:\n",
    "        bnd1,bnd2 = 0,7\n",
    "        \n",
    "    print(zepth)\n",
    "    \n",
    "    argumt=da.array.argmin(da.array.where(zepth-ENsal['DEPTH'].fillna(1e5)>=0,zepth-ENsal['DEPTH'].fillna(1e5),1e5),1).compute()\n",
    "    born1=da.array.all(zepth-ENsal['DEPTH'].ffill(dim='N_LEVELS').bfill(dim='N_LEVELS')<0,axis=1)\n",
    "    born2=da.array.all(zepth-ENsal['DEPTH'].ffill(dim='N_LEVELS').bfill(dim='N_LEVELS')>0,axis=1)\n",
    "\n",
    "    Nprs = ENsal.coords['N_PROF'].size\n",
    "    Nzz = ENsal.coords['N_LEVELS'].size\n",
    "    \n",
    "    z0,z1=ENsal['DEPTH'].data.vindex[np.linspace(0,Nprs-1,Nprs,dtype='int'),argumt],ENsal['DEPTH'].data.vindex[np.linspace(0,Nprs-1,Nprs,dtype='int'),np.where((1+argumt)>149,149,(1+argumt))]\n",
    "    quality = (ENsal['PSAL_QC']==1.)&(ENsal['PSAL_LEVEL_QC']<=2.)\n",
    "    T0,T1=ENsal['PSAL'].where(quality).data.vindex[np.linspace(0,Nprs-1,Nprs,dtype='int'),argumt],ENsal['PSAL'].where(quality).data.vindex[np.linspace(0,Nprs-1,Nprs,dtype='int'),np.where((1+argumt)>149,149,(1+argumt))]\n",
    "\n",
    "    born3 = np.array([born1, born2, (z0 <bnd1), (z1>bnd2) ,(z0 > z1) ,(zepth<z0) ,(zepth>z1)]).any(axis=0)\n",
    "\n",
    "    #     born3 = (z0-z1>=zscl)\n",
    "    T0=np.where(born3,np.nan,T0)\n",
    "    T1=np.where(born3,np.nan,T1)\n",
    "    if switch : \n",
    "        ENewtemp = np.concatenate((ENewtemp,(np.array([T0,T1,np.empty_like(T0)])[np.nanargmin(np.abs(np.array([z0,z1,1e6*np.ones_like(T0)])-zepth),0),np.ix_(np.arange(0,Nprs,1))])),axis=0)\n",
    "        # QC_z = np.concatenate((QC_z,np.where(np.isnan(T0),np.nan,(np.where(z1-z0<=zscl,0,1)+np.where(zepth-z0<zscl,0,1)+np.where(z1-zepth,0,1)))[np.newaxis,:]),axis=0)\n",
    "    if not switch: \n",
    "        ENewtemp = np.array([T0,T1,np.empty_like(T0)])[np.nanargmin(np.abs(np.array([z0,z1,1e6*np.ones_like(T0)])-zepth),0),np.ix_(np.arange(0,Nprs,1))]\n",
    "        # QC_z = np.where(np.isnan(T0),np.nan,(np.where(z1-z0<=zscl,0,1)+np.where(zepth-z0<zscl,0,1)+np.where(z1-zepth,0,1)))[np.newaxis,:]\n",
    "        switch = True\n",
    "\n",
    "       #\n",
    "#      \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea3d6f6a-38a7-4052-a2fd-932c32c9f2c4",
   "metadata": {},
   "source": [
    "# The super-profiles are created for obserations with daily frequency measurements (moorings, mostly). The observation files are saved. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "818d39f0-b5d7-4557-8aac-cd54b6a01234",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "conflicting sizes for dimension 'depth': length 6 on 'Depth' and length 10 on {'depth': 'PSAL', 'N_PROF': 'PSAL'}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_16720/1001003310.py\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m ENsal2 = xar.Dataset( \n\u001b[0m\u001b[1;32m      2\u001b[0m         {\n\u001b[1;32m      3\u001b[0m             \u001b[0;34m'PSAL'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'depth'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'N_PROF'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mENewtemp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m             \u001b[0;31m# 'QC' : (['depth','N_PROF'],QC_z.compute())\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         },\n",
      "\u001b[0;32m/opt/linux/envs/envs/phyocean-2022.05/lib/python3.8/site-packages/xarray/core/dataset.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data_vars, coords, attrs)\u001b[0m\n\u001b[1;32m    748\u001b[0m             \u001b[0mcoords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcoords\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvariables\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    749\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 750\u001b[0;31m         variables, coord_names, dims, indexes, _ = merge_data_and_coords(\n\u001b[0m\u001b[1;32m    751\u001b[0m             \u001b[0mdata_vars\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcoords\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"broadcast_equals\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    752\u001b[0m         )\n",
      "\u001b[0;32m/opt/linux/envs/envs/phyocean-2022.05/lib/python3.8/site-packages/xarray/core/merge.py\u001b[0m in \u001b[0;36mmerge_data_and_coords\u001b[0;34m(data, coords, compat, join)\u001b[0m\n\u001b[1;32m    481\u001b[0m     \u001b[0mexplicit_coords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcoords\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    482\u001b[0m     \u001b[0mindexes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_extract_indexes_from_coords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcoords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 483\u001b[0;31m     return merge_core(\n\u001b[0m\u001b[1;32m    484\u001b[0m         \u001b[0mobjects\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjoin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexplicit_coords\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexplicit_coords\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mindexes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    485\u001b[0m     )\n",
      "\u001b[0;32m/opt/linux/envs/envs/phyocean-2022.05/lib/python3.8/site-packages/xarray/core/merge.py\u001b[0m in \u001b[0;36mmerge_core\u001b[0;34m(objects, compat, join, combine_attrs, priority_arg, explicit_coords, indexes, fill_value)\u001b[0m\n\u001b[1;32m    638\u001b[0m     \u001b[0massert_unique_multiindex_level_names\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvariables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    639\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 640\u001b[0;31m     \u001b[0mdims\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcalculate_dimensions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvariables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    641\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    642\u001b[0m     \u001b[0mcoord_names\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnoncoord_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdetermine_coords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcoerced\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/linux/envs/envs/phyocean-2022.05/lib/python3.8/site-packages/xarray/core/dataset.py\u001b[0m in \u001b[0;36mcalculate_dimensions\u001b[0;34m(variables)\u001b[0m\n\u001b[1;32m    202\u001b[0m                 \u001b[0mlast_used\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mdims\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 204\u001b[0;31m                 raise ValueError(\n\u001b[0m\u001b[1;32m    205\u001b[0m                     \u001b[0;34mf\"conflicting sizes for dimension {dim!r}: \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m                     \u001b[0;34mf\"length {size} on {k!r} and length {dims[dim]} on {last_used!r}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: conflicting sizes for dimension 'depth': length 6 on 'Depth' and length 10 on {'depth': 'PSAL', 'N_PROF': 'PSAL'}"
     ]
    }
   ],
   "source": [
    "\n",
    "ENsal2 = xar.Dataset( \n",
    "        {\n",
    "            'PSAL': (['depth','N_PROF'],ENewtemp),\n",
    "            # 'QC' : (['depth','N_PROF'],QC_z.compute())\n",
    "        },\n",
    "        coords = {\n",
    "            'JULD': (['N_PROF'],ENsal['JULD'].values),\n",
    "            'LATITUDE': (['N_PROF'],ENsal['LATITUDE'].values),\n",
    "            'LONGITUDE': (['N_PROF'],ENsal['LONGITUDE'].values),\n",
    "            'Depth': (['depth'],RC['depth'].values),\n",
    "            'STATION_IDENTIFIER' : (['N_PROF'],ENsal['STATION_IDENTIFIER'].values),\n",
    "            'STATION_NUMBER' : (['N_PROF'], ENsal['STATION_TYPE'].values)\n",
    "        }\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "indexes = np.char.array(np.linspace(0,500,501,dtype='int').astype('str') ).zfill(3)\n",
    "\n",
    "indexes = np.char.array(['820'] ).zfill(3)\n",
    "\n",
    "condit = ((ENsal2['STATION_NUMBER'].dropna('N_PROF').str.strip().astype('str').isin(indexes)))\n",
    "\n",
    "\n",
    "moorObs = ENsal2.where(condit,drop=True).reset_coords(['LATITUDE','LONGITUDE']).compute()\n",
    "\n",
    "labofmoor = np.unique(moorObs['STATION_IDENTIFIER'])\n",
    "\n",
    "timebins=np.arange(np.datetime64(str(timemin)+'-01-01'),np.datetime64(str(timemax)+'-12-31')+15,15)\n",
    "# timebins=np.arange(np.datetime64('1990-01-01'),np.datetime64('1999-12-31')+15,15)\n",
    "\n",
    "if labofmoor.size!=0:\n",
    "    moorObs2 = moorObs.where(moorObs['STATION_IDENTIFIER']==labofmoor[0],drop=True).groupby_bins(group='JULD',bins=timebins).mean(skipna=True).dropna('JULD_bins',how='all').rename_dims({'JULD_bins':'N_PROF'})\n",
    "    moorObs2 = moorObs2.assign_coords({'JULD_bins':(['N_PROF'],np.asanyarray([_.mid for _ in(moorObs2['JULD_bins'].values)]))})\n",
    "\n",
    "    ii=1\n",
    "    for label in labofmoor[1:] : \n",
    "        print(str(ii)+'/'+str(labofmoor.shape),end='\\r')\n",
    "        appendix=moorObs.where(moorObs['STATION_IDENTIFIER']==label,drop=True).groupby_bins(group='JULD',bins=timebins).mean(skipna=True).dropna('JULD_bins',how='all').rename_dims({'JULD_bins':'N_PROF'})\n",
    "        appendix = appendix.assign_coords({'JULD_bins':(['N_PROF'],np.asanyarray([_.mid for _ in(appendix['JULD_bins'].values)]))})\n",
    "\n",
    "        moorObs2 =xar.concat((moorObs2,appendix),dim='N_PROF')\n",
    "        ii+=1\n",
    "\n",
    "    \n",
    "    \n",
    "    ENsal3 = xar.concat((ENsal2.where(~condit,drop=True).reset_coords(['LATITUDE','LONGITUDE']).drop(['STATION_NUMBER','STATION_IDENTIFIER']),moorObs2.rename({'JULD_bins':'JULD'})),dim='N_PROF')\n",
    "    \n",
    "else : \n",
    "    ENsal3 =ENsal2.where(~condit,drop=True).reset_coords(['LATITUDE','LONGITUDE']).drop(['STATION_NUMBER','STATION_IDENTIFIER'])\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "ENsal3.to_netcdf('ENsal_NoInterp'+str(timemin)[-2:]+'NoFMoor_OCCIPUT.nc','w')\n",
    "ENsal2.to_netcdf('ENsal_NoInterp'+str(timemin)[-2:]+'_OCCIPUT.nc','w')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "phyocean-2022.05",
   "language": "python",
   "name": "phyocean-2022.05"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
